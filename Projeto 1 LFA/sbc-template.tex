\documentclass[12pt]{article}

\usepackage{sbc-template}
\usepackage{graphicx,url}
\usepackage{listings}
\usepackage{longtable}

\usepackage[brazil]{babel}   
%\usepackage[latin1]{inputenc}  
\usepackage[utf8]{inputenc}  
% UTF-8 encoding is recommended by ShareLaTex

     
\sloppy

\title{Construção de uma aplicação para geração, determinização e minimização de Autômatos Finitos Determinísticos em linguagem C}

\author{Acácia dos C. da Terra\inst{1}, Gabriel B. Galli\inst{1}, João P. W. Bernardi\inst{1}, Vladimir Belinski\inst{1} }

\address{Ciência da Computação -- Universidade Federal da Fronteira Sul
  (UFFS)\\
  Caixa Postal 181 -- 89.802-112 -- Chapecó -- SC -- Brasil
  \email{\{terra.acacia, g7.galli96, winckler.joao, vlbelinski\}@gmail.com}
}

\begin{document} 

\maketitle

\begin{abstract}
 This paper describes the implementation of an application in C programming language for generation, determinization and minimization (without the use of equivalence classes) of Deterministic Finite Automata (DFAs) constructed from tokens and Regular Grammars (RGs). Consisting in a possible version of a lexical analyzer, the particularities and operation of the application, such as its planning and code will be demonstrated and analyzed in the work. At the end it will be checked the operation of the application by conducting tests.
\end{abstract}
     
\begin{resumo} 
 O presente trabalho descreve a implementação de uma aplicação em linguagem de programação C para geração, determinização e minimização (sem utilização de Classes de Equivalência) de Autômatos Finitos Determinísticos (AFDs) construídos a partir de tokens e Gramáticas Regulares (GRs). Consistindo em uma possível versão de analisador léxico, as particularidades e funcionamento da aplicação, tal como seu planejamento e código serão demonstrados e analisados no trabalho. No final será verificado o funcionamento da aplicação através da realização de testes.

\end{resumo}


\section{Introdução}

No presente trabalho são apresentadas as implementações de dois escalonadores em substituição ao antigo escalonador do \emph{xv6}.  A primeira abordagem, um escalonador baseado em loteria

\section{Escalonamento}

\subsection{Escalonamento por loteria}
 
Objetivo:
Construir aplicação para gerar um Autômato Finito Determinístico (AFD) a partir de tokens e
gramáticas, determinizar e minimizar o AF.
Descrição:
A aplicação faz a carga de tokens e Gramáticas Regulares (GR) a partir de um arquivo fonte (texto).
Exemplo de arquivo de entrada:
se
entao
senao
/%<S> ::= a<A> | e<A> | i<A> | o<A> | u<A>
/%<A> ::= a<A> | e<A> | i<A> | o<A> | u<A> | ε

Usar notação BNF para as GRs.
Para cada token e gramática, a aplicação gera o conjunto de transições rotuladas em um único AF
durante a carga. Para isso, compartilha apenas o estado inicial e gera estados exclusivos para as
transições dos demais símbolos dos tokens e/ou estados das GRs.
O AF será não determinístico quando ocorrer uma ou mais situações em que dois tokens ou sentenças
definidas por GR iniciam pelo mesmo símbolo.
Para os tokens e GR acima exemplificados, teremos o seguinte AFND:

Como exemplo, o AFND acima teve as transições não mapeadas preenchidas por estado de erro e a
coluna final 'x' representa todos os símbolos não pertencentes ao conjunto de símbolos de todos os
alfabetos das linguagens representadas por tokens e gramaticas. No entanto, este procedimento
(preenchimento do AF com estados de erro) só ocorre após os processos de determinização e
minimização do AF.
Então, se o processo de reconhecimento de cadeias parar (encontrar separador de token) em estados
finais, os tokens serão reconhecidos como segue:
B: se
G: entao
L: senao
M: variavel
Após a construção do AFND, aplicar o teorema de determinização para obter o AFD. A aplicação deve
permitir o acompanhamento do processo de determinização.
O AFD resultante deve ser submetido ao processo de minimização, contudo, sem aplicar Classe de
Equivalência. No AFD final os estados podem ser representados por números. Os símbolos podem ser
representados pelo correspondente numérico de acordo com a tabela ASCII.
Salvar o AFD final em arquivo de saída. Formato de livre escolha (csv, ...)

\begin{scriptsize}
\begin{verbatim}
struct proc {
  uint sz;                     // Size of process memory (bytes)
  pde_t* pgdir;                // Page table
};
\end{verbatim}
\end{scriptsize}



\section{Descrição de um possível processo de construção de analisadores léxicos}

O processo de construção de analisadores léxicos objetiva, a partir de tokens, a geração de um Autômato Finito Determinístico (AFD) mínimo e sem classes de equivalência que reconhece (ou não) tais tokens como símbolos de uma linguagem.

Para isso, inicialmente têm-se um conjunto de tokens que podem ser cadeias ou gramáticas regulares. Para cada token é realizada a construção de um autômato finito. Feito isso, é realizada a composição desses autômatos finitos gerados em um único autômato. Para tal é compartilhado apenas o estado inicial, sendo gerados estados exclusivos para as transações dos demais símbolos dos tokens e/ou estados das gramáticas regulares. O Autômato Finito (AF) gerado será não determinístico quando acontecer um ou mais casos em que dois tokens ou sentenças definidas por gramáticas regulares iniciam pelo mesmo símbolo.

Em sequência à composição dos AFs gerados em um único AF é realizada a determinização, processo pelo qul o autômato, se for um Autômato Finito Não Determinístico (AFND), será transformado em um AFD equivalente. Em seguida, ocorre a etapa de minimização, onde são eliminados estados mortos (um estado é dito morto quando a partir dele não é possível alcançar um estado final) e inalcançáveis (um estado é dito inalcançável quando a partir do estado inicial não consigo alcançar esse estado).

Todavia, não é feito uso de classes de equivalência, pois é necessário que cada estado final reconheça um conjunto de sentenças específico, sendo que a utilização de classes de equivalência poderia vir a unir estados que reconhecem sentenças diferentes, podendo a versão minimizada até mesmo reconhecer sentenças que antes não o eram.

Por fim, a respeito da construção de analisadores léxicos temos o algoritmo de análise léxica, onde existe uma condição que é utilizada na verificação de cada símbolo lido, checando-se se tal símbolo é um separador (pode ser um espaço, quebra de linha...) ou não e, caso for separador, se também é token. Nessa verificação têm-se que ao encontrar um símbolo separador é tomado o estado em que se parou e inserido esse na fita de saída. por sua vez, a fita de saída será utilizada como entrada para um possível analisador sintático. Cabe ressaltar que se o separador for também um token é necessário prosseguir na análise para se saber se de fato naquele momento de leitura o símbolo era parte de um token ou um separador.

\section{Conclusão}

Conclui-se que 

\bibliographystyle{sbc}
\bibliography{sbc-template}

\end{document}